{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing an input string is the basis needed to do higher-level operations such as tokenizing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_str = '༆ ཤི་བཀྲ་ཤིས་  tr བདེ་་ལེ གས། བཀྲ་ཤིས་བདེ་ལེགས་༡༢༣ཀཀ། མཐའི་རྒྱ་མཚོར་གནས་པའི་ཉས་ཆུ་འཐུང་།། །།མཁའ། བཀྲ ཤིས བཀྲ་ཤིས་'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pybotextchunks.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This class is a wrapper around PyBoChunk (that it subclasses).\n",
    "Its main purpose is to provide the input to be fed into the trie within tokenizer.py\n",
    "\n",
    "Here is what it produces:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The output is a <class 'list'> of <class 'tuple'> containing each 2 elements.\n",
      "\tex:  (None, (102, 0, 2))\n",
      "The first element is either a <class 'NoneType'> or a <class 'list'> of ints\n",
      "\tex: None or [2, 3]\n"
     ]
    }
   ],
   "source": [
    "from pybo import PyBoTextChunks\n",
    "text_chunks = PyBoTextChunks(input_str)\n",
    "\n",
    "output = text_chunks.serve_syls_to_trie()\n",
    "print(f'The output is a {type(output)} of {type(output[0])} containing each {len(output[0])} elements.')\n",
    "print('\\tex: ', output[0])\n",
    "print(f'The first element is either a {type(output[0][0])} or a {type(output[1][0])} of ints')\n",
    "print('\\tex:', output[0][0], 'or', output[1][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See below for the second element, it is the output of PyBoChunk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.\t None\n",
      "2.\t [2, 3]\n",
      "3.\t [5, 6, 7]\n",
      "4.\t [9, 10, 11]\n",
      "5.\t None\n",
      "6.\t [18, 19, 20]\n",
      "7.\t None\n",
      "8.\t [23, 24, 26, 27]\n",
      "9.\t None\n",
      "10.\t [30, 31, 32]\n",
      "11.\t [34, 35, 36]\n",
      "12.\t [38, 39, 40]\n",
      "13.\t [42, 43, 44, 45]\n",
      "14.\t None\n",
      "15.\t [50, 51]\n",
      "16.\t None\n",
      "17.\t [54, 55, 56, 57]\n",
      "18.\t [59, 60, 61]\n",
      "19.\t [63, 64, 65, 66]\n",
      "20.\t [68, 69, 70]\n",
      "21.\t [72, 73, 74]\n",
      "22.\t [76, 77]\n",
      "23.\t [79, 80]\n",
      "24.\t [82, 83, 84, 85]\n",
      "25.\t None\n",
      "26.\t [92, 93, 94]\n",
      "27.\t None\n",
      "28.\t [97, 98, 99, 101, 102, 103, 105, 106, 107]\n",
      "29.\t [109, 110, 111]\n"
     ]
    }
   ],
   "source": [
    "for n, trie_input in enumerate(output):\n",
    "    print(f'{n+1}.\\t {trie_input[0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first element provides the information to the trie that a given chunk is a syllable, and thus should be fed to the trie, or that it is not a syllable.\n",
    "\n",
    "In case it is a syllable, its elements are indices of the characters that constitute the syllable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t ༆ ཤི་བཀྲ་ཤིས་  tr བདེ་་ལེ གས། བཀྲ་ཤིས་བདེ་ལེགས་༡༢༣ཀཀ། མཐའི་རྒྱ་མཚོར་གནས་པའི་ཉས་ཆུ་འཐུང་།། །།མཁའ། བཀྲ ཤིས བཀྲ་ཤིས་\n",
      "1.\n",
      "2.\t['ཤ', 'ི', '་']\n",
      "3.\t['བ', 'ཀ', 'ྲ', '་']\n",
      "4.\t['ཤ', 'ི', 'ས', '་']\n",
      "5.\n",
      "6.\t['བ', 'ད', 'ེ', '་']\n",
      "7.\n",
      "8.\t['ལ', 'ེ', 'ག', 'ས', '་']\n",
      "9.\n",
      "10.\t['བ', 'ཀ', 'ྲ', '་']\n",
      "11.\t['ཤ', 'ི', 'ས', '་']\n",
      "12.\t['བ', 'ད', 'ེ', '་']\n",
      "13.\t['ལ', 'ེ', 'ག', 'ས', '་']\n",
      "14.\n",
      "15.\t['ཀ', 'ཀ', '་']\n",
      "16.\n",
      "17.\t['མ', 'ཐ', 'འ', 'ི', '་']\n",
      "18.\t['ར', 'ྒ', 'ྱ', '་']\n",
      "19.\t['མ', 'ཚ', 'ོ', 'ར', '་']\n",
      "20.\t['ག', 'ན', 'ས', '་']\n",
      "21.\t['པ', 'འ', 'ི', '་']\n",
      "22.\t['ཉ', 'ས', '་']\n",
      "23.\t['ཆ', 'ུ', '་']\n",
      "24.\t['འ', 'ཐ', 'ུ', 'ང', '་']\n",
      "25.\n",
      "26.\t['མ', 'ཁ', 'འ', '་']\n",
      "27.\n",
      "28.\t['བ', 'ཀ', 'ྲ', 'ཤ', 'ི', 'ས', 'བ', 'ཀ', 'ྲ', '་']\n",
      "29.\t['ཤ', 'ི', 'ས', '་']\n"
     ]
    }
   ],
   "source": [
    "print('\\t', input_str)\n",
    "for n, trie_input in enumerate(output):\n",
    "    syl = trie_input[0]\n",
    "    if syl:\n",
    "        syllable = [input_str[s] for s in syl] + ['་']\n",
    "        print(f'{n+1}.\\t{syllable}')\n",
    "    else:\n",
    "        print(f'{n+1}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Punctuation chunks are left aside, non-Tibetan parts also and the content of the syllable has been cleaned.\n",
    "In token 6, the double tsek has been normalized to a single tsek.\n",
    "In token 7, the space in the middle has been left aside.\n",
    "On top of this, the non-syllabic chunks give us a cue that any ongoing word has to end there.\n",
    "\n",
    "Now, this cleaned content can be fed in the trie, syllable after syllable, character after character."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pybochunk.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This class is powered by the chunking framework defined in BoChunk (that it subclasses).\n",
    "\n",
    "It implements the following chunking pipeline:\n",
    " - turn the input string into a sequence of alternating \"bo\" and \"non-bo\" chunks\n",
    " - turn the \"bo\" chunks into a sequence of alternating \"punct\" and \"bo\" chunks\n",
    " - turn the remaining \"bo\" chunks into a sequence of alternating \"sym\" and \"bo\" chunks\n",
    " - turn the remaining \"bo\" chunks into a sequence of alternating \"num\" and \"bo\" chunks\n",
    " - turn the remaining \"bo\" chunks into syllables\n",
    " - concatenate the chunks containing only spaces to the preceding one\n",
    "\n",
    "The last pipe is created by using the building blocks of BoChunk, but implemented here, since this treatment merges two chunks given a condition, while BoChunk is a chunking framework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pybo import PyBoChunk\n",
    "\n",
    "chunks = PyBoChunk(input_str)\n",
    "output = chunks.chunk()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The raw output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.\t(102, 0, 2)\n",
      "2.\t(106, 2, 3)\n",
      "3.\t(106, 5, 4)\n",
      "4.\t(106, 9, 6)\n",
      "5.\t(101, 15, 3)\n",
      "6.\t(106, 18, 4)\n",
      "7.\t(102, 22, 1)\n",
      "8.\t(106, 23, 5)\n",
      "9.\t(102, 28, 2)\n",
      "10.\t(106, 30, 4)\n",
      "11.\t(106, 34, 4)\n",
      "12.\t(106, 38, 4)\n",
      "13.\t(106, 42, 5)\n",
      "14.\t(109, 47, 3)\n",
      "15.\t(106, 50, 2)\n",
      "16.\t(102, 52, 2)\n",
      "17.\t(106, 54, 5)\n",
      "18.\t(106, 59, 4)\n",
      "19.\t(106, 63, 5)\n",
      "20.\t(106, 68, 4)\n",
      "21.\t(106, 72, 4)\n",
      "22.\t(106, 76, 3)\n",
      "23.\t(106, 79, 3)\n",
      "24.\t(106, 82, 5)\n",
      "25.\t(102, 87, 5)\n",
      "26.\t(106, 92, 3)\n",
      "27.\t(102, 95, 2)\n",
      "28.\t(106, 97, 12)\n",
      "29.\t(106, 109, 4)\n"
     ]
    }
   ],
   "source": [
    "for n, chunk in enumerate(output):\n",
    "    print(f'{n+1}.\\t{chunk}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last two elements in the tuple are:\n",
    " - the starting index of the current chunk in the input string\n",
    " - the length of the current chunk\n",
    " \n",
    "PyBoChunk provides a method to replace these two indices with the actual substring:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.\t(102, '༆ ')\n",
      "2.\t(106, 'ཤི་')\n",
      "3.\t(106, 'བཀྲ་')\n",
      "4.\t(106, 'ཤིས་  ')\n",
      "5.\t(101, 'tr ')\n",
      "6.\t(106, 'བདེ་')\n",
      "7.\t(102, '་')\n",
      "8.\t(106, 'ལེ གས')\n",
      "9.\t(102, '། ')\n",
      "10.\t(106, 'བཀྲ་')\n",
      "11.\t(106, 'ཤིས་')\n",
      "12.\t(106, 'བདེ་')\n",
      "13.\t(106, 'ལེགས་')\n",
      "14.\t(109, '༡༢༣')\n",
      "15.\t(106, 'ཀཀ')\n",
      "16.\t(102, '། ')\n",
      "17.\t(106, 'མཐའི་')\n",
      "18.\t(106, 'རྒྱ་')\n",
      "19.\t(106, 'མཚོར་')\n",
      "20.\t(106, 'གནས་')\n",
      "21.\t(106, 'པའི་')\n",
      "22.\t(106, 'ཉས་')\n",
      "23.\t(106, 'ཆུ་')\n",
      "24.\t(106, 'འཐུང་')\n",
      "25.\t(102, '།། །།')\n",
      "26.\t(106, 'མཁའ')\n",
      "27.\t(102, '། ')\n",
      "28.\t(106, 'བཀྲ ཤིས བཀྲ་')\n",
      "29.\t(106, 'ཤིས་')\n"
     ]
    }
   ],
   "source": [
    "with_substrings = chunks.get_chunked(output)\n",
    "for n, chunk in enumerate(with_substrings):\n",
    "    print(f'{n+1}.\\t{chunk}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I could do it manually, though:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"༆ \" equals \"༆ \"\n"
     ]
    }
   ],
   "source": [
    "chunk_str = input_str[output[0][1]:output[0][1]+output[0][2]]\n",
    "print(f'\"{with_substrings[0][1]}\" equals \"{chunk_str}\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's get human-readable values for the first int:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.\t('punct', '༆ ')\n",
      "2.\t('syl', 'ཤི་')\n",
      "3.\t('syl', 'བཀྲ་')\n",
      "4.\t('syl', 'ཤིས་  ')\n",
      "5.\t('non-bo', 'tr ')\n",
      "6.\t('syl', 'བདེ་')\n",
      "7.\t('punct', '་')\n",
      "8.\t('syl', 'ལེ གས')\n",
      "9.\t('punct', '། ')\n",
      "10.\t('syl', 'བཀྲ་')\n",
      "11.\t('syl', 'ཤིས་')\n",
      "12.\t('syl', 'བདེ་')\n",
      "13.\t('syl', 'ལེགས་')\n",
      "14.\t('num', '༡༢༣')\n",
      "15.\t('syl', 'ཀཀ')\n",
      "16.\t('punct', '། ')\n",
      "17.\t('syl', 'མཐའི་')\n",
      "18.\t('syl', 'རྒྱ་')\n",
      "19.\t('syl', 'མཚོར་')\n",
      "20.\t('syl', 'གནས་')\n",
      "21.\t('syl', 'པའི་')\n",
      "22.\t('syl', 'ཉས་')\n",
      "23.\t('syl', 'ཆུ་')\n",
      "24.\t('syl', 'འཐུང་')\n",
      "25.\t('punct', '།། །།')\n",
      "26.\t('syl', 'མཁའ')\n",
      "27.\t('punct', '། ')\n",
      "28.\t('syl', 'བཀྲ ཤིས བཀྲ་')\n",
      "29.\t('syl', 'ཤིས་')\n"
     ]
    }
   ],
   "source": [
    "with_markers = chunks.get_markers(with_substrings)\n",
    "for n, chunk in enumerate(with_markers):\n",
    "    print(f'{n+1}.\\t{chunk}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - punct: chunk containing Tibetan punctuation\n",
    " - syl: chunk containing a single syllable (necessarily Tibetan characters)\n",
    " - num: chunk containing Tibetan numerals\n",
    " - non-bo: chunk containing non-Tibetan characters\n",
    " \n",
    "Note that at this point, the only thing we have is substrings of the initial string with chunk markers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# bochunk.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BoChunk is a chunking framework built on top of BoString (see below).\n",
    "\n",
    "Its methods are used to create chunks – groups of characters that pertain to the same category. The produced output is a binary sequence of chunks matching or not a given condition.\n",
    "\n",
    "Although each chunking method only produces two kinds of chunks (those that match and those that don't), piped chunking allows to apply complex chunking patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pybo.bochunk import BoChunk\n",
    "    \n",
    "bc = BoChunk(input_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. The available chunking methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Either Tibetan characters or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the output of chunking functions are <class 'list'> each containing 3 <class 'int'>\n"
     ]
    }
   ],
   "source": [
    "bo_nonbo = bc.chunk_bo_chars()\n",
    "print(f'the output of chunking functions are {type(bo_nonbo)} each containing {len(bo_nonbo[0])} {type(bo_nonbo[0][0])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.\t(100, 0, 15)\n",
      "2.\t(101, 15, 2)\n",
      "3.\t(100, 17, 96)\n"
     ]
    }
   ],
   "source": [
    "for n, c in enumerate(bo_nonbo):\n",
    "    print(f'{n+1}.\\t{c}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have already seen this format in PyBoChunk, which actually only applies the chunking methods available in BoChunk in order to produce chunks corresponding to the expectations of a Tibetan reader.\n",
    "\n",
    "In a human-friendly format, it actually is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.\tbo\t\t\"༆ ཤི་བཀྲ་ཤིས་  \"\n",
      "2.\tnon-bo\t\t\"tr\"\n",
      "3.\tbo\t\t\" བདེ་་ལེ གས། བཀྲ་ཤིས་བདེ་ལེགས་༡༢༣ཀཀ། མཐའི་རྒྱ་མཚོར་གནས་པའི་ཉས་ཆུ་འཐུང་།། །།མཁའ། བཀྲ ཤིས བཀྲ་ཤིས་\"\n"
     ]
    }
   ],
   "source": [
    "for n, r in enumerate(bc.get_markers(bc.get_chunked(bo_nonbo))):\n",
    "    print(f'{n+1}.\\t{r[0]}\\t\\t\"{r[1]}\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have indeed chunked the input string is a sequence of alternatively Tibetan and non-Tibetan chunks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Either Tibetan punctuation or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.\tpunct\t\t\"༆ \"\n",
      "2.\tnon-punct\t\t\"ཤི་བཀྲ་ཤིས་\"\n",
      "3.\tpunct\t\t\"  \"\n",
      "4.\tnon-punct\t\t\"tr\"\n",
      "5.\tpunct\t\t\" \"\n",
      "6.\tnon-punct\t\t\"བདེ་\"\n",
      "7.\tpunct\t\t\"་\"\n",
      "8.\tnon-punct\t\t\"ལེ གས\"\n",
      "9.\tpunct\t\t\"། \"\n",
      "10.\tnon-punct\t\t\"བཀྲ་ཤིས་བདེ་ལེགས་༡༢༣ཀཀ\"\n",
      "11.\tpunct\t\t\"། \"\n",
      "12.\tnon-punct\t\t\"མཐའི་རྒྱ་མཚོར་གནས་པའི་ཉས་ཆུ་འཐུང་\"\n",
      "13.\tpunct\t\t\"།། །།\"\n",
      "14.\tnon-punct\t\t\"མཁའ\"\n",
      "15.\tpunct\t\t\"། \"\n",
      "16.\tnon-punct\t\t\"བཀྲ ཤིས བཀྲ་ཤིས་\"\n"
     ]
    }
   ],
   "source": [
    "punct_nonpunct = bc.chunk_punct()\n",
    "\n",
    "for n, p in enumerate(bc.get_markers(bc.get_chunked(punct_nonpunct))):\n",
    "    print(f'{n+1}.\\t{p[0]}\\t\\t\"{p[1]}\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we have an alternating sequence of chunks that only cares about Tibetan punctuation.\n",
    "\n",
    "Note how \"tr\" is simply considered to be non-punct."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Either Tibetan symbols or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.\tnon-sym\t\t\"༆ ཤི་བཀྲ་ཤིས་  tr བདེ་་ལེ གས། བཀྲ་ཤིས་བདེ་ལེགས་༡༢༣ཀཀ། མཐའི་རྒྱ་མཚོར་གནས་པའི་ཉས་ཆུ་འཐུང་།། །།མཁའ། བཀྲ ཤིས བཀྲ་ཤིས་\"\n"
     ]
    }
   ],
   "source": [
    "sym_nonsym = bc.chunk_symbol()\n",
    "\n",
    "for n, s in enumerate(bc.get_markers(bc.get_chunked(sym_nonsym))):\n",
    "    print(f'{n+1}.\\t{s[0]}\\t\\t\"{s[1]}\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... there were no Tibetan symbols in the input string."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Either Tibetan digits or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.\tnon-num\t\t\"༆ ཤི་བཀྲ་ཤིས་  tr བདེ་་ལེ གས། བཀྲ་ཤིས་བདེ་ལེགས་\"\n",
      "2.\tnum\t\t\"༡༢༣\"\n",
      "3.\tnon-num\t\t\"ཀཀ། མཐའི་རྒྱ་མཚོར་གནས་པའི་ཉས་ཆུ་འཐུང་།། །།མཁའ། བཀྲ ཤིས བཀྲ་ཤིས་\"\n"
     ]
    }
   ],
   "source": [
    "num_nonnum = bc.chunk_number()\n",
    "\n",
    "for n, m in enumerate(bc.get_markers(bc.get_chunked(num_nonnum))):\n",
    "    print(f'{n+1}.\\t{m[0]}\\t\\t\"{m[1]}\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have correctly identified the Tibetan digits in the input string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Either spaces or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.\tnon-space\t\t\"༆\"\n",
      "2.\tspace\t\t\" \"\n",
      "3.\tnon-space\t\t\"ཤི་བཀྲ་ཤིས་\"\n",
      "4.\tspace\t\t\"  \"\n",
      "5.\tnon-space\t\t\"tr\"\n",
      "6.\tspace\t\t\" \"\n",
      "7.\tnon-space\t\t\"བདེ་་ལེ\"\n",
      "8.\tspace\t\t\" \"\n",
      "9.\tnon-space\t\t\"གས།\"\n",
      "10.\tspace\t\t\" \"\n",
      "11.\tnon-space\t\t\"བཀྲ་ཤིས་བདེ་ལེགས་༡༢༣ཀཀ།\"\n",
      "12.\tspace\t\t\" \"\n",
      "13.\tnon-space\t\t\"མཐའི་རྒྱ་མཚོར་གནས་པའི་ཉས་ཆུ་འཐུང་།།\"\n",
      "14.\tspace\t\t\" \"\n",
      "15.\tnon-space\t\t\"།།མཁའ།\"\n",
      "16.\tspace\t\t\" \"\n",
      "17.\tnon-space\t\t\"བཀྲ\"\n",
      "18.\tspace\t\t\" \"\n",
      "19.\tnon-space\t\t\"ཤིས\"\n",
      "20.\tspace\t\t\" \"\n",
      "21.\tnon-space\t\t\"བཀྲ་ཤིས་\"\n"
     ]
    }
   ],
   "source": [
    "space_nonspace = bc.chunk_spaces()\n",
    "\n",
    "for n, s in enumerate(bc.get_markers(bc.get_chunked(space_nonspace))):\n",
    "    print(f'{n+1}.\\t{s[0]}\\t\\t\"{s[1]}\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Syllabify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.\tsyl\t\t\"༆ ཤི་\"\n",
      "2.\tsyl\t\t\"བཀྲ་\"\n",
      "3.\tsyl\t\t\"ཤིས་\"\n",
      "4.\tsyl\t\t\"  tr བདེ་་\"\n",
      "5.\tsyl\t\t\"ལེ གས། བཀྲ་\"\n",
      "6.\tsyl\t\t\"ཤིས་\"\n",
      "7.\tsyl\t\t\"བདེ་\"\n",
      "8.\tsyl\t\t\"ལེགས་\"\n",
      "9.\tsyl\t\t\"༡༢༣ཀཀ། མཐའི་\"\n",
      "10.\tsyl\t\t\"རྒྱ་\"\n",
      "11.\tsyl\t\t\"མཚོར་\"\n",
      "12.\tsyl\t\t\"གནས་\"\n",
      "13.\tsyl\t\t\"པའི་\"\n",
      "14.\tsyl\t\t\"ཉས་\"\n",
      "15.\tsyl\t\t\"ཆུ་\"\n",
      "16.\tsyl\t\t\"འཐུང་\"\n",
      "17.\tsyl\t\t\"།། །།མཁའ། བཀྲ ཤིས བཀྲ་\"\n",
      "18.\tsyl\t\t\"ཤིས་\"\n"
     ]
    }
   ],
   "source": [
    "syl_nonsyl = bc.syllabify()\n",
    "\n",
    "for n, s in enumerate(bc.get_markers(bc.get_chunked(syl_nonsyl))):\n",
    "    print(f'{n+1}.\\t{s[0]}\\t\\t\"{s[1]}\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the only chunking method that doesn't create a binary sequence of chunks matching or not a given condition. \n",
    "\n",
    "Instead, it breaks up the input it receives into syllables that are only separated by tseks(both regular and non-breaking). For this reason, syllabify takes for granted that the input it receives is only Tibetan characters that can compose a syllable.\n",
    "\n",
    "When that was the case, for example in chunks 10 to 16, the syllabation is operated as expected.\n",
    "\n",
    "Spaces, on the other hand, are allowed within a syllable because they only serve to \"beautify\" a tibetan text by visually marking the end of a clause or that of a sentence after a punctuation. So we reproduce the behavior of a Tibetan reader, who will simply bypass a space encountered anywhere else."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extending the framework with additional methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All of these chunking methods follow a simple standardized format, making it extremely simple to create new ones to suit any specific needs. They are in turn built on top of BoString, which relies on the groups of characters that were put together to reproduce the intuition of a native Tibetan who is reading a text.\n",
    "\n",
    "So in case finer chunking abilities are required, finer character groups should be created in BoString."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Piped Chunking: Implementing complex chunking patterns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applying successively the chunking methods we have seen above in a specific order allows to design complex patterns for chunking a given string.\n",
    "\n",
    "We will exemplify this functionality by reproducing the chunking pipeline that PyBoChunk implements:\n",
    "\n",
    "    a. turn the input string into a sequence of alternating \"bo\" / \"non-bo\" chunks\n",
    "    b. turn the \"bo\" chunks into a sequence of alternating \"punct\" / \"bo\" chunks\n",
    "    c. turn the remaining \"bo\" chunks into a sequence of alternating \"sym\" / \"bo\" chunks\n",
    "    d. turn the remaining \"bo\" chunks into a sequence of alternating \"num\" / \"bo\" chunks\n",
    "    e. turn the \"bo\" chunks into a sequence of syllables\n",
    "    f. concatenate the chunks containing only spaces to the preceding one"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a. Either \"bo\" or \"non-bo\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t ༆ ཤི་བཀྲ་ཤིས་  tr བདེ་་ལེ གས། བཀྲ་ཤིས་བདེ་ལེགས་༡༢༣ཀཀ། མཐའི་རྒྱ་མཚོར་གནས་པའི་ཉས་ཆུ་འཐུང་།། །།མཁའ། བཀྲ ཤིས བཀྲ་ཤིས་\n",
      "\n",
      "1.\tbo\t\t\"༆ ཤི་བཀྲ་ཤིས་  \"\n",
      "2.\tnon-bo\t\t\"tr\"\n",
      "3.\tbo\t\t\" བདེ་་ལེ གས། བཀྲ་ཤིས་བདེ་ལེགས་༡༢༣ཀཀ། མཐའི་རྒྱ་མཚོར་གནས་པའི་ཉས་ཆུ་འཐུང་།། །།མཁའ། བཀྲ ཤིས བཀྲ་ཤིས་\"\n"
     ]
    }
   ],
   "source": [
    "chunks = bc.chunk_bo_chars()\n",
    "\n",
    "print('\\t', input_str, end='\\n\\n')\n",
    "for n, c in enumerate(bc.get_markers(bc.get_chunked(chunks))):\n",
    "    print(f'{n+1}.\\t{c[0]}\\t\\t\"{c[1]}\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output is similar to what we had above. Yet this time, instead of rechunking the whole input string in \"punct\" or \"non-punct\", we will use piped chunking to only apply chunk_punct() to the \"bo\" chunks.\n",
    "\n",
    "Important: piped chunking is only possible when the content of the chunks is a tuple of ints."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b. Either \"punct\" or \"bo\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t ༆ ཤི་བཀྲ་ཤིས་  tr བདེ་་ལེ གས། བཀྲ་ཤིས་བདེ་ལེགས་༡༢༣ཀཀ། མཐའི་རྒྱ་མཚོར་གནས་པའི་ཉས་ཆུ་འཐུང་།། །།མཁའ། བཀྲ ཤིས བཀྲ་ཤིས་\n",
      "\n",
      "1.\tpunct\t\t\"༆ \"\n",
      "2.\tbo\t\t\"ཤི་བཀྲ་ཤིས་\"\n",
      "3.\tpunct\t\t\"  \"\n",
      "4.\tnon-bo\t\t\"tr\"\n",
      "5.\tpunct\t\t\" \"\n",
      "6.\tbo\t\t\"བདེ་\"\n",
      "7.\tpunct\t\t\"་\"\n",
      "8.\tbo\t\t\"ལེ གས\"\n",
      "9.\tpunct\t\t\"། \"\n",
      "10.\tbo\t\t\"བཀྲ་ཤིས་བདེ་ལེགས་༡༢༣ཀཀ\"\n",
      "11.\tpunct\t\t\"། \"\n",
      "12.\tbo\t\t\"མཐའི་རྒྱ་མཚོར་གནས་པའི་ཉས་ཆུ་འཐུང་\"\n",
      "13.\tpunct\t\t\"།། །།\"\n",
      "14.\tbo\t\t\"མཁའ\"\n",
      "15.\tpunct\t\t\"། \"\n",
      "16.\tbo\t\t\"བཀྲ ཤིས བཀྲ་ཤིས་\"\n"
     ]
    }
   ],
   "source": [
    "bc.pipe_chunk(chunks, bc.chunk_punct, to_chunk=bc.BO_MARKER, yes=bc.PUNCT_MARKER)\n",
    "\n",
    "print('\\t', input_str, end='\\n\\n')\n",
    "for n, c in enumerate(bc.get_markers(bc.get_chunked(chunks))):\n",
    "    print(f'{n+1}.\\t{c[0]}\\t\\t\"{c[1]}\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pipe_chunk() takes as arguments:\n",
    " - chunks: the chunks produced by a previous chunking methods. They are expected to be a list of tuples containing each three ints.\n",
    " - bc.chunk_punct: the chunking method we want to apply on top of the existing chunks\n",
    " - to_chunk: the marker that identifies which chunks should be further processed by the new chunking method\n",
    " - yes: the marker to be used on the new chunks that match the new chunking method. (the chunks that don't match simply retain their previous marker)\n",
    "\n",
    "So to put it into simple English: \n",
    "\n",
    "                  \"Within the existing chunks, I would like to take those marked as 'bo' \n",
    "                  and within them separate what is actual Tibetan text ('bo') \n",
    "                  from what is Tibetan punctuation ('punct')\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### c. Either \"sym\" or \"bo\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t ༆ ཤི་བཀྲ་ཤིས་  tr བདེ་་ལེ གས། བཀྲ་ཤིས་བདེ་ལེགས་༡༢༣ཀཀ། མཐའི་རྒྱ་མཚོར་གནས་པའི་ཉས་ཆུ་འཐུང་།། །།མཁའ། བཀྲ ཤིས བཀྲ་ཤིས་\n",
      "\n",
      "1.\tpunct\t\t\"༆ \"\n",
      "2.\tbo\t\t\"ཤི་བཀྲ་ཤིས་\"\n",
      "3.\tpunct\t\t\"  \"\n",
      "4.\tnon-bo\t\t\"tr\"\n",
      "5.\tpunct\t\t\" \"\n",
      "6.\tbo\t\t\"བདེ་\"\n",
      "7.\tpunct\t\t\"་\"\n",
      "8.\tbo\t\t\"ལེ གས\"\n",
      "9.\tpunct\t\t\"། \"\n",
      "10.\tbo\t\t\"བཀྲ་ཤིས་བདེ་ལེགས་༡༢༣ཀཀ\"\n",
      "11.\tpunct\t\t\"། \"\n",
      "12.\tbo\t\t\"མཐའི་རྒྱ་མཚོར་གནས་པའི་ཉས་ཆུ་འཐུང་\"\n",
      "13.\tpunct\t\t\"།། །།\"\n",
      "14.\tbo\t\t\"མཁའ\"\n",
      "15.\tpunct\t\t\"། \"\n",
      "16.\tbo\t\t\"བཀྲ ཤིས བཀྲ་ཤིས་\"\n"
     ]
    }
   ],
   "source": [
    "bc.pipe_chunk(chunks, bc.chunk_symbol, to_chunk=bc.BO_MARKER, yes=bc.SYMBOL_MARKER)\n",
    "\n",
    "print('\\t', input_str, end='\\n\\n')\n",
    "for n, c in enumerate(bc.get_markers(bc.get_chunked(chunks))):\n",
    "    print(f'{n+1}.\\t{c[0]}\\t\\t\"{c[1]}\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There were no symbols, so the chunks are unchanged."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### d. Either \"num\" or \"bo\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t ༆ ཤི་བཀྲ་ཤིས་  tr བདེ་་ལེ གས། བཀྲ་ཤིས་བདེ་ལེགས་༡༢༣ཀཀ། མཐའི་རྒྱ་མཚོར་གནས་པའི་ཉས་ཆུ་འཐུང་།། །།མཁའ། བཀྲ ཤིས བཀྲ་ཤིས་\n",
      "\n",
      "1.\tpunct\t\t\"༆ \"\n",
      "2.\tbo\t\t\"ཤི་བཀྲ་ཤིས་\"\n",
      "3.\tpunct\t\t\"  \"\n",
      "4.\tnon-bo\t\t\"tr\"\n",
      "5.\tpunct\t\t\" \"\n",
      "6.\tbo\t\t\"བདེ་\"\n",
      "7.\tpunct\t\t\"་\"\n",
      "8.\tbo\t\t\"ལེ གས\"\n",
      "9.\tpunct\t\t\"། \"\n",
      "10.\tbo\t\t\"བཀྲ་ཤིས་བདེ་ལེགས་\"\n",
      "11.\tnum\t\t\"༡༢༣\"\n",
      "12.\tbo\t\t\"ཀཀ\"\n",
      "13.\tpunct\t\t\"། \"\n",
      "14.\tbo\t\t\"མཐའི་རྒྱ་མཚོར་གནས་པའི་ཉས་ཆུ་འཐུང་\"\n",
      "15.\tpunct\t\t\"།། །།\"\n",
      "16.\tbo\t\t\"མཁའ\"\n",
      "17.\tpunct\t\t\"། \"\n",
      "18.\tbo\t\t\"བཀྲ ཤིས བཀྲ་ཤིས་\"\n"
     ]
    }
   ],
   "source": [
    "bc.pipe_chunk(chunks, bc.chunk_number, to_chunk=bc.BO_MARKER, yes=bc.NUMBER_MARKER)\n",
    "\n",
    "print('\\t', input_str, end='\\n\\n')\n",
    "for n, c in enumerate(bc.get_markers(bc.get_chunked(chunks))):\n",
    "    print(f'{n+1}.\\t{c[0]}\\t\\t\"{c[1]}\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "                  \"Within the existing chunks, I would like to take those marked as 'bo' \n",
    "                  and within them separate what is actual Tibetan text ('bo') \n",
    "                  from what is Tibetan digits ('num')\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### e. Syllabify \"bo\" into \"syl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t ༆ ཤི་བཀྲ་ཤིས་  tr བདེ་་ལེ གས། བཀྲ་ཤིས་བདེ་ལེགས་༡༢༣ཀཀ། མཐའི་རྒྱ་མཚོར་གནས་པའི་ཉས་ཆུ་འཐུང་།། །།མཁའ། བཀྲ ཤིས བཀྲ་ཤིས་\n",
      "\n",
      "1.\tpunct\t\t\"༆ \"\n",
      "2.\tsyl\t\t\"ཤི་\"\n",
      "3.\tsyl\t\t\"བཀྲ་\"\n",
      "4.\tsyl\t\t\"ཤིས་\"\n",
      "5.\tpunct\t\t\"  \"\n",
      "6.\tnon-bo\t\t\"tr\"\n",
      "7.\tpunct\t\t\" \"\n",
      "8.\tsyl\t\t\"བདེ་\"\n",
      "9.\tpunct\t\t\"་\"\n",
      "10.\tsyl\t\t\"ལེ གས\"\n",
      "11.\tpunct\t\t\"། \"\n",
      "12.\tsyl\t\t\"བཀྲ་\"\n",
      "13.\tsyl\t\t\"ཤིས་\"\n",
      "14.\tsyl\t\t\"བདེ་\"\n",
      "15.\tsyl\t\t\"ལེགས་\"\n",
      "16.\tnum\t\t\"༡༢༣\"\n",
      "17.\tsyl\t\t\"ཀཀ\"\n",
      "18.\tpunct\t\t\"། \"\n",
      "19.\tsyl\t\t\"མཐའི་\"\n",
      "20.\tsyl\t\t\"རྒྱ་\"\n",
      "21.\tsyl\t\t\"མཚོར་\"\n",
      "22.\tsyl\t\t\"གནས་\"\n",
      "23.\tsyl\t\t\"པའི་\"\n",
      "24.\tsyl\t\t\"ཉས་\"\n",
      "25.\tsyl\t\t\"ཆུ་\"\n",
      "26.\tsyl\t\t\"འཐུང་\"\n",
      "27.\tpunct\t\t\"།། །།\"\n",
      "28.\tsyl\t\t\"མཁའ\"\n",
      "29.\tpunct\t\t\"། \"\n",
      "30.\tsyl\t\t\"བཀྲ ཤིས བཀྲ་\"\n",
      "31.\tsyl\t\t\"ཤིས་\"\n"
     ]
    }
   ],
   "source": [
    "bc.pipe_chunk(chunks, bc.syllabify, to_chunk=bc.BO_MARKER, yes=bc.SYL_MARKER)\n",
    "\n",
    "print('\\t', input_str, end='\\n\\n')\n",
    "for n, c in enumerate(bc.get_markers(bc.get_chunked(chunks))):\n",
    "    print(f'{n+1}.\\t{c[0]}\\t\\t\"{c[1]}\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we are ! We have successfully preprocessed the input string into a sequence of identified content that we can confidently use for any further NLP processing.\n",
    "\n",
    "In order to have the exact same output as PyBoChunk, chunk 5 should be attached to chunk 4 and chunk 7 to chunk 6. Since this is implemented as a private method of PyBoChunk, we will stop this demonstration here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# bostring.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
