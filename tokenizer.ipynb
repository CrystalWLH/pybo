{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# PyBo's Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. Running the tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pybo import BoTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instanciate the tokenizer with the 'POS' profile (see [profile documentation](this.file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Trie...\n",
      "Time: 2.5575690269470215\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BoTokenizer('POS')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given a random text in Tibetan language,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_str = '༆ ཤི་བཀྲ་ཤིས་  tr བདེ་་ལེ གས། བཀྲ་ཤིས་བདེ་ལེགས་༡༢༣ཀཀ མཐའི་རྒྱ་མཚོར་གནས་པའི་ཉས་ཆུ་འཐུང་།། །།'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what information can be derived from it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The output is a <class 'list'>.\n",
      "The constituting elements are <class 'pybo.token.Token'>s.\n"
     ]
    }
   ],
   "source": [
    "tokens = tokenizer.tokenize(input_str)\n",
    "print(f'The output is a {type(tokens)}.\\nThe constituting elements are {type(tokens[0])}s.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. A first look"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Non Tibetan tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First thing, I see there is non-Tibetan stuff in the middle of the input string. Let's see how I can detect it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"tr\", token number 4, is not Tibetan.\n",
      "this starts at 15th character in the input and spans 2 characters\n"
     ]
    }
   ],
   "source": [
    "for n, token in enumerate(tokens):\n",
    "    if token.type == 'non-bo':\n",
    "        content = token.content\n",
    "        print(f'\"{content}\", token number {n+1}, is not Tibetan.')\n",
    "        start = token.start\n",
    "        length = token.len\n",
    "        print(f'this starts at {start}th character in the input and spans {length} characters')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokens that are not words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is there any Tibetan punctuation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"༆\", token number 1, is a punctuation token.\n",
      "\"།\", token number 6, is a punctuation token.\n",
      "\"།། \", token number 19, is a punctuation token.\n",
      "\"།།\", token number 20, is a punctuation token.\n"
     ]
    }
   ],
   "source": [
    "for n, token in enumerate(tokens):\n",
    "    if token.type == 'punct':\n",
    "        content = token.content\n",
    "        print(f'\"{content}\", token number {n+1}, is a punctuation token.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How are the Tibetan digits treated?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"༡༢༣\", token number 9, is a numeral.\n"
     ]
    }
   ],
   "source": [
    "for n, token in enumerate(tokens):\n",
    "    if token.type == 'num':\n",
    "        content = token.content\n",
    "        print(f'\"{content}\", token number {n+1}, is a numeral.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. The attributes of tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Strictly speaking, a token is a word that has been correctly extracted from the input string, but our Token objects have much more information that is awaiting to be exploited by NLP treatments:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Token.content: the unmodified content straight from the input string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "token 1: \"༆\"\n",
      "token 2: \" ཤི་\"\n",
      "token 3: \"བཀྲ་ཤིས་  \"\n",
      "token 4: \"tr\"\n",
      "token 5: \" བདེ་་ལེ གས\"\n",
      "token 6: \"།\"\n",
      "token 7: \" བཀྲ་ཤིས་\"\n",
      "token 8: \"བདེ་ལེགས་\"\n",
      "token 9: \"༡༢༣\"\n",
      "token 10: \"ཀཀ མཐའི་\"\n",
      "token 11: \"རྒྱ་མཚོ\"\n",
      "token 12: \"ར་\"\n",
      "token 13: \"གནས་པ\"\n",
      "token 14: \"འི་\"\n",
      "token 15: \"ཉ\"\n",
      "token 16: \"ས་\"\n",
      "token 17: \"ཆུ་\"\n",
      "token 18: \"འཐུང་\"\n",
      "token 19: \"།། \"\n",
      "token 20: \"།།\"\n"
     ]
    }
   ],
   "source": [
    "for n, token in enumerate(tokens):\n",
    "    print(f'token {n+1}: \"{token.content}\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Token.type: the basic types of tokens "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "punct\t: token 1(\"༆\")\n",
      "syl\t: token 2(\" ཤི་\")\n",
      "syl\t: token 3(\"བཀྲ་ཤིས་  \")\n",
      "non-bo\t: token 4(\"tr\")\n",
      "syl\t: token 5(\" བདེ་་ལེ གས\")\n",
      "punct\t: token 6(\"།\")\n",
      "syl\t: token 7(\" བཀྲ་ཤིས་\")\n",
      "syl\t: token 8(\"བདེ་ལེགས་\")\n",
      "num\t: token 9(\"༡༢༣\")\n",
      "syl\t: token 10(\"ཀཀ མཐའི་\")\n",
      "syl\t: token 11(\"རྒྱ་མཚོ\")\n",
      "syl\t: token 12(\"ར་\")\n",
      "syl\t: token 13(\"གནས་པ\")\n",
      "syl\t: token 14(\"འི་\")\n",
      "syl\t: token 15(\"ཉ\")\n",
      "syl\t: token 16(\"ས་\")\n",
      "syl\t: token 17(\"ཆུ་\")\n",
      "syl\t: token 18(\"འཐུང་\")\n",
      "punct\t: token 19(\"།། \")\n",
      "punct\t: token 20(\"།།\")\n"
     ]
    }
   ],
   "source": [
    "for n, token in enumerate(tokens):\n",
    "    print(f'{token.type}\\t: token {n+1}(\"{token.content}\")')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(there is a bug in token 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "legend:\n",
    " - syl: contains valid Tibetan syllables\n",
    " - num: Tibetan numerals\n",
    " - punct: Tibetan punctuation\n",
    " - non-bo: non-Tibetan content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
