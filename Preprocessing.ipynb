{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing an input string is the basis needed to do higher-level operations such as tokenizing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_str = '༆ ཤི་བཀྲ་ཤིས་  tr བདེ་་ལེ གས། བཀྲ་ཤིས་བདེ་ལེགས་༡༢༣ཀཀ། མཐའི་རྒྱ་མཚོར་གནས་པའི་ཉས་ཆུ་འཐུང་།། །།མཁའ།'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pybotextchunks.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This class is a wrapper around PyBoChunk (that it subclasses).\n",
    "Its main purpose is to provide the input to be fed into the trie within tokenizer.py\n",
    "\n",
    "Here is what it produces:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The output is a <class 'list'> of <class 'tuple'> containing each 2 elements.\n",
      "\tex:  (None, (102, 0, 2))\n",
      "The first element is either a <class 'NoneType'> or a <class 'list'> of ints\n",
      "\tex: None or [2, 3]\n"
     ]
    }
   ],
   "source": [
    "from pybo import PyBoTextChunks\n",
    "text_chunks = PyBoTextChunks(input_str)\n",
    "\n",
    "output = text_chunks.serve_syls_to_trie()\n",
    "print(f'The output is a {type(output)} of {type(output[0])} containing each {len(output[0])} elements.')\n",
    "print('\\tex: ', output[0])\n",
    "print(f'The first element is either a {type(output[0][0])} or a {type(output[1][0])} of ints')\n",
    "print('\\tex:', output[0][0], 'or', output[1][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See below for the second element, it is the output of PyBoChunk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.\t None\n",
      "2.\t [2, 3]\n",
      "3.\t [5, 6, 7]\n",
      "4.\t [9, 10, 11]\n",
      "5.\t None\n",
      "6.\t [18, 19, 20]\n",
      "7.\t [23, 24, 26, 27]\n",
      "8.\t None\n",
      "9.\t [30, 31, 32]\n",
      "10.\t [34, 35, 36]\n",
      "11.\t [38, 39, 40]\n",
      "12.\t [42, 43, 44, 45]\n",
      "13.\t None\n",
      "14.\t [50, 51]\n",
      "15.\t None\n",
      "16.\t [54, 55, 56, 57]\n",
      "17.\t [59, 60, 61]\n",
      "18.\t [63, 64, 65, 66]\n",
      "19.\t [68, 69, 70]\n",
      "20.\t [72, 73, 74]\n",
      "21.\t [76, 77]\n",
      "22.\t [79, 80]\n",
      "23.\t [82, 83, 84, 85]\n",
      "24.\t None\n",
      "25.\t [92, 93, 94]\n",
      "26.\t None\n"
     ]
    }
   ],
   "source": [
    "for n, trie_input in enumerate(output):\n",
    "    print(f'{n+1}.\\t {trie_input[0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first element provides the information to the trie that a given chunk is a syllable, and thus should be fed to the trie, or that it is not a syllable.\n",
    "\n",
    "In case it is a syllable, its elements are indices of the characters that constitute the syllable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t ༆ ཤི་བཀྲ་ཤིས་  tr བདེ་་ལེ གས། བཀྲ་ཤིས་བདེ་ལེགས་༡༢༣ཀཀ། མཐའི་རྒྱ་མཚོར་གནས་པའི་ཉས་ཆུ་འཐུང་།། །།མཁའ།\n",
      "1.\n",
      "2.\t['ཤ', 'ི', '་']\n",
      "3.\t['བ', 'ཀ', 'ྲ', '་']\n",
      "4.\t['ཤ', 'ི', 'ས', '་']\n",
      "5.\n",
      "6.\t['བ', 'ད', 'ེ', '་']\n",
      "7.\t['ལ', 'ེ', 'ག', 'ས', '་']\n",
      "8.\n",
      "9.\t['བ', 'ཀ', 'ྲ', '་']\n",
      "10.\t['ཤ', 'ི', 'ས', '་']\n",
      "11.\t['བ', 'ད', 'ེ', '་']\n",
      "12.\t['ལ', 'ེ', 'ག', 'ས', '་']\n",
      "13.\n",
      "14.\t['ཀ', 'ཀ', '་']\n",
      "15.\n",
      "16.\t['མ', 'ཐ', 'འ', 'ི', '་']\n",
      "17.\t['ར', 'ྒ', 'ྱ', '་']\n",
      "18.\t['མ', 'ཚ', 'ོ', 'ར', '་']\n",
      "19.\t['ག', 'ན', 'ས', '་']\n",
      "20.\t['པ', 'འ', 'ི', '་']\n",
      "21.\t['ཉ', 'ས', '་']\n",
      "22.\t['ཆ', 'ུ', '་']\n",
      "23.\t['འ', 'ཐ', 'ུ', 'ང', '་']\n",
      "24.\n",
      "25.\t['མ', 'ཁ', 'འ', '་']\n",
      "26.\n"
     ]
    }
   ],
   "source": [
    "print('\\t', input_str)\n",
    "for n, trie_input in enumerate(output):\n",
    "    syl = trie_input[0]\n",
    "    if syl:\n",
    "        syllable = [input_str[s] for s in syl] + ['་']\n",
    "        print(f'{n+1}.\\t{syllable}')\n",
    "    else:\n",
    "        print(f'{n+1}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Punctuation chunks are left aside, non-Tibetan parts also and the content of the syllable has been cleaned.\n",
    "In token 6, the double tsek has been normalized to a single tsek.\n",
    "In token 7, the space in the middle has been left aside.\n",
    "On top of this, the non-syllabic chunks give us a cue that any ongoing word has to end there.\n",
    "\n",
    "Now, this cleaned content can be fed in the trie, syllable after syllable, character after character."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pybochunk.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This class is powered by the chunking framework defined in BoChunk (that it subclasses).\n",
    "\n",
    "It implements the following chunking pipeline:\n",
    " - turn the input string into a sequence of alternating \"bo\" and \"non-bo\" chunks\n",
    " - turn the \"bo\" chunks into a sequence of alternating \"punct\" and \"bo\" chunks\n",
    " - turn the \"bo\" chunks into a sequence of syllables\n",
    " - concatenate the chunks containing only spaces to the preceding one\n",
    "\n",
    "The last pipe is created by using the building blocks of BoChunk, but implemented here, since this treatment is specific to Tibetan language. Following Tibetan usage, spaces are not considered to be a punctuation mark. They are simply attached to the chunk preceding them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pybo import PyBoChunk\n",
    "\n",
    "chunks = PyBoChunk(input_str)\n",
    "output = chunks.chunk()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The raw output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.\t(102, 0, 2)\n",
      "2.\t(106, 2, 3)\n",
      "3.\t(106, 5, 4)\n",
      "4.\t(106, 9, 6)\n",
      "5.\t(101, 15, 3)\n",
      "6.\t(106, 18, 5)\n",
      "7.\t(106, 23, 5)\n",
      "8.\t(102, 28, 2)\n",
      "9.\t(106, 30, 4)\n",
      "10.\t(106, 34, 4)\n",
      "11.\t(106, 38, 4)\n",
      "12.\t(106, 42, 5)\n",
      "13.\t(109, 47, 3)\n",
      "14.\t(106, 50, 2)\n",
      "15.\t(102, 52, 2)\n",
      "16.\t(106, 54, 5)\n",
      "17.\t(106, 59, 4)\n",
      "18.\t(106, 63, 5)\n",
      "19.\t(106, 68, 4)\n",
      "20.\t(106, 72, 4)\n",
      "21.\t(106, 76, 3)\n",
      "22.\t(106, 79, 3)\n",
      "23.\t(106, 82, 5)\n",
      "24.\t(102, 87, 5)\n",
      "25.\t(106, 92, 3)\n",
      "26.\t(102, 95, 1)\n"
     ]
    }
   ],
   "source": [
    "for n, chunk in enumerate(output):\n",
    "    print(f'{n+1}.\\t{chunk}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last two elements in the tuple are:\n",
    " - the starting index of the current chunk in the input string\n",
    " - the length of the current chunk\n",
    " \n",
    "PyBoChunk provides a method to replace these two indices with the actual substring:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.\t(102, '༆ ')\n",
      "2.\t(106, 'ཤི་')\n",
      "3.\t(106, 'བཀྲ་')\n",
      "4.\t(106, 'ཤིས་  ')\n",
      "5.\t(101, 'tr ')\n",
      "6.\t(106, 'བདེ་་')\n",
      "7.\t(106, 'ལེ གས')\n",
      "8.\t(102, '། ')\n",
      "9.\t(106, 'བཀྲ་')\n",
      "10.\t(106, 'ཤིས་')\n",
      "11.\t(106, 'བདེ་')\n",
      "12.\t(106, 'ལེགས་')\n",
      "13.\t(109, '༡༢༣')\n",
      "14.\t(106, 'ཀཀ')\n",
      "15.\t(102, '། ')\n",
      "16.\t(106, 'མཐའི་')\n",
      "17.\t(106, 'རྒྱ་')\n",
      "18.\t(106, 'མཚོར་')\n",
      "19.\t(106, 'གནས་')\n",
      "20.\t(106, 'པའི་')\n",
      "21.\t(106, 'ཉས་')\n",
      "22.\t(106, 'ཆུ་')\n",
      "23.\t(106, 'འཐུང་')\n",
      "24.\t(102, '།། །།')\n",
      "25.\t(106, 'མཁའ')\n",
      "26.\t(102, '།')\n"
     ]
    }
   ],
   "source": [
    "with_substrings = chunks.get_chunked(output)\n",
    "for n, chunk in enumerate(with_substrings):\n",
    "    print(f'{n+1}.\\t{chunk}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I could do it manually, though:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"༆ \" equals \"༆ \"\n"
     ]
    }
   ],
   "source": [
    "chunk_str = input_str[output[0][1]:output[0][1]+output[0][2]]\n",
    "print(f'\"{with_substrings[0][1]}\" equals \"{chunk_str}\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's get human-readable values for the first int:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.\t('punct', '༆ ')\n",
      "2.\t('syl', 'ཤི་')\n",
      "3.\t('syl', 'བཀྲ་')\n",
      "4.\t('syl', 'ཤིས་  ')\n",
      "5.\t('non-bo', 'tr ')\n",
      "6.\t('syl', 'བདེ་་')\n",
      "7.\t('syl', 'ལེ གས')\n",
      "8.\t('punct', '། ')\n",
      "9.\t('syl', 'བཀྲ་')\n",
      "10.\t('syl', 'ཤིས་')\n",
      "11.\t('syl', 'བདེ་')\n",
      "12.\t('syl', 'ལེགས་')\n",
      "13.\t('num', '༡༢༣')\n",
      "14.\t('syl', 'ཀཀ')\n",
      "15.\t('punct', '། ')\n",
      "16.\t('syl', 'མཐའི་')\n",
      "17.\t('syl', 'རྒྱ་')\n",
      "18.\t('syl', 'མཚོར་')\n",
      "19.\t('syl', 'གནས་')\n",
      "20.\t('syl', 'པའི་')\n",
      "21.\t('syl', 'ཉས་')\n",
      "22.\t('syl', 'ཆུ་')\n",
      "23.\t('syl', 'འཐུང་')\n",
      "24.\t('punct', '།། །།')\n",
      "25.\t('syl', 'མཁའ')\n",
      "26.\t('punct', '།')\n"
     ]
    }
   ],
   "source": [
    "with_markers = chunks.get_markers(with_substrings)\n",
    "for n, chunk in enumerate(with_markers):\n",
    "    print(f'{n+1}.\\t{chunk}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - punct: chunk containing Tibetan punctuation\n",
    " - syl: chunk containing a single syllable (necessarily Tibetan characters)\n",
    " - num: chunk containing Tibetan numerals\n",
    " - non-bo: chunk containing non-Tibetan characters\n",
    " \n",
    "Note that at this point, the only thing we have is substrings of the initial string with chunk markers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# bochunk.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    from pybo.bochunk import BoChunk\n",
    "\n",
    "    bo_str = ' བཀྲ་ཤིས་  tr བདེ་ལེགས།'\n",
    "    bc = BoChunk(bo_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # 1. Initial chunking\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    >>> chunks = bc.chunk_bo_chars()  # uses default chunk-marks here, but override them in the pipeline\n",
    "\n",
    "    >>> chunks  # actual chunks. each chunk is tuple containing: (chunk-marker, starting_index, chunk_length)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    >>> bc.get_markers(chunks)  # shows the human-readable description of the constant\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    >>> bc.get_chunked(chunks)  # shows the substring for each chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # 2. First piped chunking: re-chunks tibetan chunks into tibetan text and tibetan punctuation.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    >>> bc.pipe_chunk(chunks, bc.chunk_punct, to_chunk=bc.BO_MARKER, yes=bc.PUNCT_MARKER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    >>> bc.get_markers(chunks)\n",
    "    [('bo', 0, 11), ('non-bo', 11, 2), ('bo', 13, 9), ('punct', 22, 1)]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    >>> bc.get_chunked(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # 3. Second piped chunking: re-chunks tibetan text into syllables, keeping the same chunk-marker.\n",
    "    >>> bc.pipe_chunk(chunks, bc.syllabify, to_chunk=bc.BO_MARKER, yes=bc.BO_MARKER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    >>> bc.get_markers(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    >>> bc.get_chunked(chunks)\n",
    "    [(100, ' བཀྲ་'), (100, 'ཤིས་'), (100, '  '), (101, 'tr'), (100, ' བདེ་'), (100, 'ལེགས'), (102, '།')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # 4. Formatting the resultant chunks into an easily usable structure\n",
    "    >>> chunks = bc.get_markers(chunks)         # exchange the constants for the human-readable description\n",
    "    >>> final_result = bc.get_chunked(chunks)   # exchange the indices for the substrings of each chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
